{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Projekat 3 - Mnozenje matrica.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMZm0YTUOnR3"
      },
      "source": [
        "# Paralelni algoritmi - treći projekat\n",
        "\n",
        "1. [5 bodova] Program koji vrši množenja matrica malih dimenzija (množenje se može izvršiti upotrebom jednom bloka niti)\n",
        "2. [5 bodova] Probram koji vrši množenje matrica većih dimenzija, upotrebom većeg broja CUDA blokova.\n",
        "3. [7 bodova] Ubrzati rešenje iz stavke 2 upotrebom deljene memorije (tako da niti jednog bloka prvo dovuku deo podataka u deljenu memeorju, a potom sve čitaju iz deljene memorije)\n",
        "4. [8 bodova] Izmeniti rešenje iz tačke 3 tako da se pri množenju druga matrica transponuje ($Rezultat = A \\cdot B^T$)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZQuEf7PQH8R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32ddee66-c016-49f7-e05a-74839ba94d18"
      },
      "source": [
        "!pip install pycuda"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pycuda\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/61/47d3235a4c13eec5a5f03594ddb268f4858734e02980afbcd806e6242fa5/pycuda-2020.1.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 5.5MB/s \n",
            "\u001b[?25hCollecting pytools>=2011.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/ed/f4b298876b9b624150cc01830075f7cb0b9e09c1abfc46daef14811f3eed/pytools-2020.4.4.tar.gz (61kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from pycuda) (4.4.2)\n",
            "Collecting appdirs>=1.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl\n",
            "Collecting mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/37/0e706200d22172eb8fa17d68a7ae22dec7631a0a92266634fb518a88a5b2/Mako-1.1.3-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (1.19.4)\n",
            "Requirement already satisfied: dataclasses>=0.7 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (0.8)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from mako->pycuda) (1.1.1)\n",
            "Building wheels for collected packages: pycuda, pytools\n",
            "  Building wheel for pycuda (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2020.1-cp36-cp36m-linux_x86_64.whl size=620705 sha256=477c596ebf32abeda41ce351f77ec196bddabe2fe4487d1a00a6d615a82e2897\n",
            "  Stored in directory: /root/.cache/pip/wheels/8f/78/d1/5bb826f81d9d490297a348d818ff3ee6dd6f2075b06dde6ea0\n",
            "  Building wheel for pytools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytools: filename=pytools-2020.4.4-py2.py3-none-any.whl size=59109 sha256=207cbd1931e468881692ba5de71fbd8393b8aafd3cd94c51c356e967830f8650\n",
            "  Stored in directory: /root/.cache/pip/wheels/6f/2a/6e/0b210d900f2b6caca133395d2fa153d8a9c03ad42c3c5d55bf\n",
            "Successfully built pycuda pytools\n",
            "Installing collected packages: appdirs, pytools, mako, pycuda\n",
            "Successfully installed appdirs-1.4.4 mako-1.1.3 pycuda-2020.1 pytools-2020.4.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbVXz0u5Yi6O",
        "outputId": "d09ea365-7382-4329-90ae-a2084ff533bf"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Dec 24 20:46:30 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.27.04    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkeHqAokYp1J"
      },
      "source": [
        "1. [5 bodova] Program koji vrši množenja matrica malih dimenzija (množenje se može izvršiti upotrebom jednom bloka niti)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQSQM6KGffzo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afde2488-3caf-48ad-ca05-79eb518ea160"
      },
      "source": [
        "import pycuda.driver as cuda\r\n",
        "import pycuda.autoinit\r\n",
        "from pycuda.compiler import SourceModule\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "mod = SourceModule(\"\"\"\r\n",
        "      __global__ void oneBlockMulti(float *a, float *b, float *c, int kolA, int kolB) {\r\n",
        "\r\n",
        "        int indexC = threadIdx.x + threadIdx.y*kolB;\r\n",
        "        int indexA = threadIdx.y*kolA;\r\n",
        "        int indexB = threadIdx.x;\r\n",
        "        float fValue = 0;\r\n",
        "        int i = 0;\r\n",
        "        for (i; i < kolA; i++) {\r\n",
        "          float aElement = a[indexA + i];\r\n",
        "          float bElement = b[indexB];\r\n",
        "          fValue += aElement * bElement;\r\n",
        "          indexB += kolB;\r\n",
        "        }\r\n",
        "        c[indexC] = fValue;\r\n",
        "      }\r\n",
        "      \"\"\")\r\n",
        "\r\n",
        "a = np.random.randn(20,15).astype(dtype=np.float32)\r\n",
        "b = np.random.randn(15,10).astype(dtype=np.float32)\r\n",
        "\r\n",
        "res_check = a.dot(b) # Rezultat za proveru\r\n",
        "c = np.zeros_like(res_check)\r\n",
        "\r\n",
        "a_gpu = cuda.mem_alloc(a.nbytes)\r\n",
        "cuda.memcpy_htod(a_gpu, a)\r\n",
        "\r\n",
        "b_gpu = cuda.mem_alloc(b.nbytes)\r\n",
        "cuda.memcpy_htod(b_gpu, b)\r\n",
        "\r\n",
        "c_gpu = cuda.mem_alloc(c.nbytes)\r\n",
        "cuda.memcpy_htod(c_gpu, c)\r\n",
        "\r\n",
        "func = mod.get_function(\"oneBlockMulti\")\r\n",
        "func(a_gpu, b_gpu, c_gpu, np.int32(a.shape[1]), np.int32(b.shape[1]), block=(np.int(b.shape[1]), np.int(a.shape[0]), 1), grid=(1, 1, 1))\r\n",
        "\r\n",
        "cuda.memcpy_dtoh(c, c_gpu)\r\n",
        "\r\n",
        "if (c==res_check).all():\r\n",
        "  print('Jednake matrice')\r\n",
        "  print(c)\r\n",
        "else:\r\n",
        "  print('Nejednake!')\r\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Jednake matrice\n",
            "[[-3.5522156   9.903658   -0.9676281   4.330498    3.0890806   3.4862545\n",
            "  -4.4934382   1.0412718  -6.4077115  -1.5505251 ]\n",
            " [ 2.913807    4.2815156   1.9980567  -0.48132947 -4.2101917   0.06444133\n",
            "  -2.4775846   4.169553   -0.7243547  -1.4836429 ]\n",
            " [ 5.6167545   2.7104738   2.4914436  -1.2026819  -3.54126     3.3654313\n",
            "  -1.9746261   3.583166   -1.180192    0.97441244]\n",
            " [ 1.4959023   2.6264045  -3.6920369   5.4260235   5.2990913   5.6880565\n",
            "  -4.1103992   2.3620925  -0.45145452  1.7505283 ]\n",
            " [ 1.3784122   3.2323766  -1.487074    5.4414415   2.9284792  -1.0043005\n",
            "  -2.9614975  -1.6788462   1.7889653   4.1380525 ]\n",
            " [ 8.031756    2.9292958   0.15897255 -0.31539473 -1.5239409  -0.40969536\n",
            "  -1.62107     4.3733873  -0.52170813  0.75442123]\n",
            " [ 3.1193721   0.6397917  -3.2804976   2.7520983  -1.751309   -1.5874898\n",
            "  -3.6179986  -3.5494227   3.776781   -4.5270977 ]\n",
            " [ 3.7440426  -0.48518825  4.505292   -0.78590703 -3.8492115   1.2985326\n",
            "  -0.78580767 -0.33166093  0.83081084 -0.9283492 ]\n",
            " [ 0.94163185 -9.294973   -0.80329955  3.1085544   6.824287   -0.73317504\n",
            "   1.0165833   4.0648813   2.0965006  -4.222541  ]\n",
            " [-2.2030447  -8.499215    0.4818366   6.7981668   5.5064826   5.008568\n",
            "  -7.2141676   7.006733   -3.6612985   5.843012  ]\n",
            " [-0.26724395 -4.8788257  -3.1168897   5.8229527   6.0318184  10.633389\n",
            "  -4.8653393  -0.46462822 -2.5854354  -5.7228765 ]\n",
            " [-2.8596747   0.7709324  -0.6861346   1.2477564  -0.66264236 -1.7817677\n",
            "   0.06736534 -3.1658933  -2.0277078  -2.0350351 ]\n",
            " [-5.6643906   7.1514554  -0.60564256 -2.9357824   0.8494784   3.7369204\n",
            "   7.582876   -0.4677998   1.0109615  -4.0330625 ]\n",
            " [-1.1329212  -0.60911614 -1.756008    0.5476792   0.7634457  -2.8046687\n",
            "   0.17608039 -0.5378932   2.9411001   2.3883932 ]\n",
            " [-4.4243813   1.5901502   1.9555677  -2.4265454  -1.9758334  -3.7429311\n",
            "   5.2039723   1.6931226   2.42559     6.2896976 ]\n",
            " [-3.1553297   4.748345   -1.3217242  -0.7143242  -0.04271462  0.24835488\n",
            "   3.7934     -3.540291   -1.1841136  -1.2901503 ]\n",
            " [-1.0056758   0.46748054  4.24275    -0.55880076  1.2276533   3.4019597\n",
            "  -5.6022615  -0.389084    2.887847    9.9616785 ]\n",
            " [ 0.858276   -1.1992626   3.7539058  -0.8590272  -4.162842    0.5355513\n",
            "   0.32012767  1.8153608  -2.423334    6.857391  ]\n",
            " [-1.2349021  -3.937257    1.1672612   5.976877    5.88704     7.4834023\n",
            "  -5.3940945  -0.69798326 -3.103164   -2.5778706 ]\n",
            " [ 3.0856268  -1.851212   -0.08298991  0.9721128   0.6836878  -3.4733431\n",
            "  -1.120904    2.1933243   1.4427309  -2.088706  ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: The CUDA compiler succeeded, but said the following:\n",
            "kernel.cu(10): warning: expression has no effect\n",
            "\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3jElL9pepMg"
      },
      "source": [
        "2. [5 bodova] Probram koji vrši množenje matrica većih dimenzija, upotrebom većeg broja CUDA blokova."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8rsTEO9ewFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49102dd7-b002-4aaf-c17e-b76eb7f7811e"
      },
      "source": [
        "import pycuda.driver as cuda\r\n",
        "import pycuda.autoinit\r\n",
        "from pycuda.compiler import SourceModule\r\n",
        "import numpy as np\r\n",
        "import math\r\n",
        "\r\n",
        "mod = SourceModule(\"\"\"\r\n",
        "\r\n",
        "      __global__ void multiBlockMultiply(float *a, float *b, float *c, int kolA, int kolB, int redA, int redB) {\r\n",
        "\r\n",
        "        long indexC = threadIdx.x + blockDim.x * blockIdx.x + threadIdx.y * kolB + blockDim.y * blockIdx.y * kolB;\r\n",
        "        long indexA = (threadIdx.y + blockDim.y * blockIdx.y) * kolA;\r\n",
        "        long indexB = blockDim.x * blockIdx.x + threadIdx.x;\r\n",
        "\r\n",
        "        if ((indexB < kolB) && (threadIdx.y + blockDim.y * blockIdx.y < redA)) {\r\n",
        "\r\n",
        "          float fValue = 0;\r\n",
        "          \r\n",
        "          for (int i = 0; i < kolA; i++) {\r\n",
        "            float aElement = a[indexA + i];\r\n",
        "            float bElement = b[indexB];\r\n",
        "            fValue += aElement * bElement;\r\n",
        "            indexB += kolB;\r\n",
        "          }\r\n",
        "\r\n",
        "          c[indexC] = fValue;\r\n",
        "\r\n",
        "        }\r\n",
        "\r\n",
        "      }\r\n",
        "\r\n",
        "      \"\"\")\r\n",
        "\r\n",
        "a = np.random.randn(64, 64).astype(dtype=np.float32)\r\n",
        "b = np.random.randn(64, 256).astype(dtype=np.float32)\r\n",
        "a = 100*a\r\n",
        "b = 100*b\r\n",
        "a = np.round(a, 1)\r\n",
        "b = np.round(b, 1)\r\n",
        "\r\n",
        "res_check = a.dot(b)\r\n",
        "res_check = np.round(res_check, 1)\r\n",
        "c = np.zeros_like(res_check)\r\n",
        "\r\n",
        "a_gpu = cuda.mem_alloc(a.nbytes)\r\n",
        "cuda.memcpy_htod(a_gpu, a)\r\n",
        "\r\n",
        "b_gpu = cuda.mem_alloc(b.nbytes)\r\n",
        "cuda.memcpy_htod(b_gpu, b)\r\n",
        "\r\n",
        "c_gpu = cuda.mem_alloc(c.nbytes)\r\n",
        "cuda.memcpy_htod(c_gpu, c)\r\n",
        "\r\n",
        "func = mod.get_function(\"multiBlockMultiply\")\r\n",
        "func(a_gpu, b_gpu, c_gpu, \r\n",
        "     np.int32(a.shape[1]), np.int32(b.shape[1]), \r\n",
        "     np.int32(a.shape[0]), np.int32(b.shape[0]), \r\n",
        "     block=(32, 32, 1), \r\n",
        "     grid=(math.ceil(b.shape[1]/32),\r\n",
        "           math.ceil(a.shape[0]/32), 1))\r\n",
        "\r\n",
        "cuda.memcpy_dtoh(c, c_gpu)\r\n",
        "c = np.round(c, 1)\r\n",
        "\r\n",
        "print('res_check**************************************************')\r\n",
        "print(res_check)\r\n",
        "print('c**********************************************************')\r\n",
        "print(c)\r\n",
        "\r\n",
        "\r\n",
        "if (c==res_check).all():\r\n",
        "  print(\"Jednake\")\r\n",
        "  print(c)\r\n",
        "else:\r\n",
        "  print(\"Nisu jednake\")\r\n",
        "  print(c-res_check)"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "res_check**************************************************\n",
            "[[ -27923.7   -1010.8  -69580.2 ...  -91267.8 -143228.5   69323.7]\n",
            " [  48605.2   68017.7   26686.1 ...   31133.4 -148783.7   30821.7]\n",
            " [  73308.   -10169.9  -32331.5 ... -114710.4   70981.4   31109.6]\n",
            " ...\n",
            " [  60242.6  -30405.2 -139434.4 ...  -55799.8  151600.2 -186146.1]\n",
            " [-119911.4  -94329.8   28492.1 ...   -5379.3    8451.9  -40511.1]\n",
            " [  47623.3  -91119.3   60742.8 ...   14518.9  -10603.4   43904. ]]\n",
            "c**********************************************************\n",
            "[[ -27923.7   -1010.8  -69580.2 ...  -91267.8 -143228.5   69323.6]\n",
            " [  48605.2   68017.7   26686.1 ...   31133.4 -148783.7   30821.7]\n",
            " [  73308.   -10169.9  -32331.5 ... -114710.4   70981.4   31109.6]\n",
            " ...\n",
            " [  60242.6  -30405.2 -139434.4 ...  -55799.8  151600.2 -186146.2]\n",
            " [-119911.3  -94329.8   28492.1 ...   -5379.3    8451.9  -40511.1]\n",
            " [  47623.3  -91119.3   60742.8 ...   14518.9  -10603.4   43904. ]]\n",
            "Nisu jednake\n",
            "[[ 0.         0.         0.        ...  0.         0.        -0.1015625]\n",
            " [ 0.         0.         0.        ...  0.         0.         0.       ]\n",
            " [ 0.         0.         0.        ...  0.         0.         0.       ]\n",
            " ...\n",
            " [ 0.         0.         0.        ...  0.         0.        -0.109375 ]\n",
            " [ 0.1015625  0.         0.        ...  0.         0.         0.       ]\n",
            " [ 0.         0.         0.        ...  0.         0.         0.       ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yzn-9hhzIkGg"
      },
      "source": [
        "3. [7 bodova] Ubrzati rešenje iz stavke 2 upotrebom deljene memorije (tako da niti jednog bloka prvo dovuku deo podataka u deljenu memoriju, a potom sve čitaju iz deljene memorije)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7uZYnPBIrS6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a0d02d6-99f3-4846-af71-0e65300c69c6"
      },
      "source": [
        "import pycuda.driver as cuda\r\n",
        "import pycuda.autoinit\r\n",
        "from pycuda.compiler import SourceModule\r\n",
        "import numpy as np\r\n",
        "import math\r\n",
        "\r\n",
        "\r\n",
        "mod = SourceModule(\"\"\"\r\n",
        "    __global__ void multiply(float *a, float *b, float *c, int n, int m, int k, int iter) {\r\n",
        "        float sol = 0;\r\n",
        "        __shared__ float dataA[1024];\r\n",
        "        __shared__ float dataB[1024];\r\n",
        "\r\n",
        "        for(int i=0; i<iter; i++) {\r\n",
        "            long ida = blockIdx.y*blockDim.y*m + threadIdx.y*m + i*blockDim.x + threadIdx.x;\r\n",
        "            long idb = i*blockDim.y*k + threadIdx.y*k + blockIdx.x * blockDim.x + threadIdx.x;\r\n",
        "\r\n",
        "            if ((i*blockDim.x + threadIdx.x < m) && (blockIdx.y*blockDim.y + threadIdx.y < n))\r\n",
        "                dataA[threadIdx.x + threadIdx.y*blockDim.x] = a[ida];\r\n",
        "            \r\n",
        "            if ((blockIdx.x * blockDim.x + threadIdx.x < k) && (i*blockDim.y + threadIdx.y < m))\r\n",
        "                dataB[threadIdx.x + threadIdx.y*blockDim.x] = b[idb];\r\n",
        "\r\n",
        "            __syncthreads();\r\n",
        "\r\n",
        "            int dif = m - i*32;\r\n",
        "            if (dif > 32)\r\n",
        "                dif = 32;\r\n",
        "            for(int j=0; j<32; j++) {\r\n",
        "                if (j < dif)\r\n",
        "                    sol += (dataA[threadIdx.y*blockDim.x + j] * dataB[threadIdx.x + j*blockDim.x]);\r\n",
        "            }\r\n",
        "            __syncthreads();\r\n",
        "        }\r\n",
        "\r\n",
        "\r\n",
        "        long idc = blockIdx.y*blockDim.y*k + threadIdx.y * k + blockIdx.x * blockDim.x + threadIdx.x;\r\n",
        "        if ((blockIdx.x * blockDim.x + threadIdx.x < k) && (blockIdx.y*blockDim.y + threadIdx.y < n))\r\n",
        "            c[idc] = sol;\r\n",
        "    }\r\n",
        "\"\"\")\r\n",
        "\r\n",
        "a = np.random.randn(40, 70).astype(dtype=np.float32)\r\n",
        "b = np.random.randn(70, 100).astype(dtype=np.float32)\r\n",
        "a = 100*a\r\n",
        "b = 100*b\r\n",
        "a = np.round(a, 1)\r\n",
        "b = np.round(b, 1)\r\n",
        "\r\n",
        "res_check = a.dot(b)\r\n",
        "res_check = np.round(res_check, 1)\r\n",
        "c = np.zeros_like(res_check).astype(dtype=np.float32)\r\n",
        "c = np.round(c, 1)\r\n",
        "\r\n",
        "a_gpu = cuda.mem_alloc(a.nbytes)\r\n",
        "cuda.memcpy_htod(a_gpu, a)\r\n",
        "\r\n",
        "b_gpu = cuda.mem_alloc(b.nbytes)\r\n",
        "cuda.memcpy_htod(b_gpu, b)\r\n",
        "\r\n",
        "c_gpu = cuda.mem_alloc(c.nbytes)\r\n",
        "cuda.memcpy_htod(c_gpu, c)\r\n",
        "\r\n",
        "func = mod.get_function(\"multiply\")\r\n",
        "func(a_gpu, b_gpu, c_gpu, np.int32(c.shape[0]), np.int32(b.shape[0]), np.int32(c.shape[1]), np.int32(math.ceil(b.shape[0]/32)),\r\n",
        "     block=(32, 32, 1),\r\n",
        "     grid=(math.ceil(c.shape[1]/32), math.ceil(c.shape[0]/32), 1))\r\n",
        "\r\n",
        "cuda.memcpy_dtoh(c, c_gpu)\r\n",
        "c = np.round(c, 1)\r\n",
        "\r\n",
        "print('res_check**************************************************')\r\n",
        "print(res_check)\r\n",
        "print('c**********************************************************')\r\n",
        "print(c)\r\n",
        "\r\n",
        "if (c==res_check).all():\r\n",
        "  print(\"Jednake\")\r\n",
        "  print(c)\r\n",
        "else:\r\n",
        "  print(\"Nisu jednake\")\r\n",
        "  print(c-res_check)"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "res_check**************************************************\n",
            "[[-130656.8   57289.2   15643.4 ...  -75679.6   99708.3   63267.5]\n",
            " [ -55643.7 -161859.5  -67817.  ...   79874.2  -65483.3   17052.7]\n",
            " [ -68607.7  -10756.   -35301.6 ... -139987.    96119.9   45369.3]\n",
            " ...\n",
            " [ -57547.7   29481.4  -48225.4 ...  -33405.8  -24940.3   -2849.6]\n",
            " [  25865.   -59423.3  -10340.9 ... -149667.   -45692.1  -59406.2]\n",
            " [  27791.1  -39780.5   33127.9 ...  -87989.2   84758.3   13696.7]]\n",
            "c**********************************************************\n",
            "[[-130656.8   57289.2   15643.3 ...  -75679.6   99708.3   63267.5]\n",
            " [ -55643.7 -161859.5  -67817.  ...   79874.2  -65483.3   17052.7]\n",
            " [ -68607.7  -10756.1  -35301.6 ... -139987.    96119.9   45369.3]\n",
            " ...\n",
            " [ -57547.8   29481.5  -48225.4 ...  -33405.8  -24940.3   -2849.6]\n",
            " [  25865.   -59423.3  -10340.9 ... -149667.   -45692.1  -59406.2]\n",
            " [  27791.1  -39780.5   33127.9 ...  -87989.2   84758.3   13696.7]]\n",
            "Nisu jednake\n",
            "[[ 0.          0.         -0.10058594 ...  0.          0.\n",
            "   0.        ]\n",
            " [ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]\n",
            " [ 0.         -0.09960938  0.         ...  0.          0.\n",
            "   0.        ]\n",
            " ...\n",
            " [-0.1015625   0.09960938  0.         ...  0.          0.\n",
            "   0.        ]\n",
            " [ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]\n",
            " [ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDuXeuNIcd0w"
      },
      "source": [
        "4. [8 bodova] Izmeniti rešenje iz tačke 3 tako da se pri množenju druga matrica transponuje ($Rezultat = A \\cdot B^T$)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vflbG-L7cgFe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2988af4b-cf7f-4646-d332-049a7ed35d12"
      },
      "source": [
        "import pycuda.driver as cuda\r\n",
        "import pycuda.autoinit\r\n",
        "from pycuda.compiler import SourceModule\r\n",
        "import numpy as np\r\n",
        "import math\r\n",
        "\r\n",
        "mod = SourceModule(\"\"\"\r\n",
        "\r\n",
        "    __global__ void multiplyTransposed(float *a, float *b, float *c, int n, int m, int k) {\r\n",
        "        float sol = 0;\r\n",
        "        __shared__ float dataA[1024];\r\n",
        "        __shared__ float dataB[1024];\r\n",
        "\r\n",
        "        for(int i=0; i<m; i++) {\r\n",
        "            long ida = blockIdx.y*blockDim.y*m + threadIdx.y*m + i*blockDim.x + threadIdx.x;\r\n",
        "            long idb = blockIdx.x*blockDim.y*m + threadIdx.y*m + i*blockDim.x + threadIdx.x;\r\n",
        "\r\n",
        "            if ((i*blockDim.x + threadIdx.x < m) && (blockIdx.y*blockDim.y + threadIdx.y < n))\r\n",
        "                dataA[threadIdx.x + threadIdx.y*blockDim.x] = a[ida];\r\n",
        "            \r\n",
        "            if ((i*blockDim.x + threadIdx.x < m) && (blockIdx.x*blockDim.y + threadIdx.y < k))\r\n",
        "                dataB[threadIdx.x + threadIdx.y*blockDim.x] = b[idb];\r\n",
        "\r\n",
        "            __syncthreads();\r\n",
        "\r\n",
        "            int dif = m - i*32;\r\n",
        "            if (dif > 32)\r\n",
        "                dif = 32;\r\n",
        "            for(int j=0; j<32; j++) {\r\n",
        "                if (j < dif)\r\n",
        "                    sol += (dataA[threadIdx.y*blockDim.x + j] * dataB[threadIdx.x*blockDim.x + j]);\r\n",
        "            }\r\n",
        "\r\n",
        "            __syncthreads();\r\n",
        "        }\r\n",
        "\r\n",
        "        long idc = blockIdx.y*blockDim.y*k + threadIdx.y * k + blockIdx.x * blockDim.x + threadIdx.x;\r\n",
        "        if ((blockIdx.x * blockDim.x + threadIdx.x < k) && (blockIdx.y*blockDim.y + threadIdx.y < n))\r\n",
        "            c[idc] = sol;\r\n",
        "    }\r\n",
        "\r\n",
        "\"\"\")\r\n",
        "\r\n",
        "a = np.random.randn(200, 64).astype(dtype=np.float32)\r\n",
        "b = np.random.randn(300, 64).astype(dtype=np.float32)\r\n",
        "a = np.round(a, 1)\r\n",
        "b = np.round(b, 1)\r\n",
        "c = np.zeros(shape=(a.shape[0], b.shape[0])).astype(dtype=np.float32)\r\n",
        "c = np.round(c, 1)\r\n",
        "\r\n",
        "a_gpu = cuda.mem_alloc(a.nbytes)\r\n",
        "cuda.memcpy_htod(a_gpu, a)\r\n",
        "\r\n",
        "b_gpu = cuda.mem_alloc(b.nbytes)\r\n",
        "cuda.memcpy_htod(b_gpu, b)\r\n",
        "\r\n",
        "c_gpu = cuda.mem_alloc(c.nbytes)\r\n",
        "cuda.memcpy_htod(c_gpu, c)\r\n",
        "\r\n",
        "bt = np.copy(b)\r\n",
        "bt = np.transpose(bt)\r\n",
        "\r\n",
        "res_check = a.dot(bt)\r\n",
        "res_check = np.round(res_check, 1)\r\n",
        "\r\n",
        "func = mod.get_function(\"multiplyTransposed\")\r\n",
        "func(a_gpu, b_gpu, c_gpu, np.int32(c.shape[0]), np.int32(b.shape[1]), np.int32(c.shape[1]), block=(32, 32, 1), grid=(math.ceil(c.shape[1]/32), math.ceil(c.shape[0]/32), 1))\r\n",
        "\r\n",
        "cuda.memcpy_dtoh(c, c_gpu)\r\n",
        "c = np.round(c, 1)\r\n",
        "\r\n",
        "print('res_check**************************************************')\r\n",
        "print(res_check)\r\n",
        "print('c**********************************************************')\r\n",
        "print(c)\r\n",
        "\r\n",
        "if (c==res_check).all():\r\n",
        "  print(\"Jednake\")\r\n",
        "  print(c)\r\n",
        "else:\r\n",
        "  print(\"Nisu jednake\")\r\n",
        "  print(c-res_check)"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "res_check**************************************************\n",
            "[[ -2.8  -1.3  14.9 ...   7.    6.1 -14.4]\n",
            " [ -1.1  -2.3  -9.1 ...  -4.2  -6.8  -1.6]\n",
            " [  4.8 -14.1  -5.4 ...   0.7 -14.6  -4.3]\n",
            " ...\n",
            " [-11.3   0.1  -8.4 ...   2.5  -0.5   6.1]\n",
            " [ -1.9  10.7  -7.2 ...  -1.2  13.9  -2.4]\n",
            " [-10.8  -5.9  11.5 ... -14.3   0.7 -12.3]]\n",
            "c**********************************************************\n",
            "[[ -2.8  -1.3  14.9 ...   7.    6.1 -14.4]\n",
            " [ -1.1  -2.3  -9.1 ...  -4.2  -6.8  -1.6]\n",
            " [  4.8 -14.1  -5.4 ...   0.7 -14.6  -4.3]\n",
            " ...\n",
            " [-11.3   0.1  -8.4 ...   2.5  -0.5   6.1]\n",
            " [ -1.9  10.7  -7.2 ...  -1.2  13.9  -2.4]\n",
            " [-10.8  -5.9  11.5 ... -14.3   0.7 -12.3]]\n",
            "Nisu jednake\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}